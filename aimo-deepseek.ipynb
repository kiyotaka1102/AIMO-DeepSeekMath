{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d161afeb",
   "metadata": {
    "papermill": {
     "duration": 0.00723,
     "end_time": "2024-06-15T12:40:02.145796",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.138566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Motivation:*\n",
    "\n",
    "### Most forked notebooks had unorganized code structure, import libraries were scattered in different places. So I organized functions into one class, removed unnecessary stuff, in hope it makes the system more understandable and give people flexibility to edit. Suggestions and fixes are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6d49b",
   "metadata": {
    "papermill": {
     "duration": 0.00642,
     "end_time": "2024-06-15T12:40:02.158962",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.152542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded10db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:40:02.174024Z",
     "iopub.status.busy": "2024-06-15T12:40:02.173385Z",
     "iopub.status.idle": "2024-06-15T12:40:02.177442Z",
     "shell.execute_reply": "2024-06-15T12:40:02.176728Z"
    },
    "papermill": {
     "duration": 0.013769,
     "end_time": "2024-06-15T12:40:02.179344",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.165575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n",
    "# https://www.kaggle.com/code/dnyaneshwalwadkar/submission-with-the-best-nb-new-api\n",
    "# https://www.kaggle.com/code/utsavsinghal2604/natural-language-and-code-integration\n",
    "# https://www.kaggle.com/code/yuanwangzhang/updated-code-interpretation-n-repetitions-17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad268f",
   "metadata": {
    "papermill": {
     "duration": 0.00671,
     "end_time": "2024-06-15T12:40:02.192717",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.186007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03376b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:40:02.207122Z",
     "iopub.status.busy": "2024-06-15T12:40:02.206890Z",
     "iopub.status.idle": "2024-06-15T12:40:02.216252Z",
     "shell.execute_reply": "2024-06-15T12:40:02.215459Z"
    },
    "papermill": {
     "duration": 0.018801,
     "end_time": "2024-06-15T12:40:02.218198",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.199397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NOTEBOOK_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9128e7",
   "metadata": {
    "papermill": {
     "duration": 0.006373,
     "end_time": "2024-06-15T12:40:02.231252",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.224879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b03e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:40:02.245828Z",
     "iopub.status.busy": "2024-06-15T12:40:02.245204Z",
     "iopub.status.idle": "2024-06-15T12:41:11.247733Z",
     "shell.execute_reply": "2024-06-15T12:41:11.246520Z"
    },
    "papermill": {
     "duration": 69.012265,
     "end_time": "2024-06-15T12:41:11.250108",
     "exception": false,
     "start_time": "2024-06-15T12:40:02.237843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/accelerate-0-29-3/accelerate-0.29.3-py3-none-any.whl -qq\n",
    "!pip install -U /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f7b1",
   "metadata": {
    "papermill": {
     "duration": 0.0065,
     "end_time": "2024-06-15T12:41:11.263806",
     "exception": false,
     "start_time": "2024-06-15T12:41:11.257306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876f592e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:11.279413Z",
     "iopub.status.busy": "2024-06-15T12:41:11.279129Z",
     "iopub.status.idle": "2024-06-15T12:41:18.109018Z",
     "shell.execute_reply": "2024-06-15T12:41:18.108234Z"
    },
    "papermill": {
     "duration": 6.840803,
     "end_time": "2024-06-15T12:41:18.111235",
     "exception": false,
     "start_time": "2024-06-15T12:41:11.270432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffea36",
   "metadata": {
    "papermill": {
     "duration": 0.006646,
     "end_time": "2024-06-15T12:41:18.124922",
     "exception": false,
     "start_time": "2024-06-15T12:41:18.118276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# New API initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c397b19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:18.139829Z",
     "iopub.status.busy": "2024-06-15T12:41:18.139428Z",
     "iopub.status.idle": "2024-06-15T12:41:18.168089Z",
     "shell.execute_reply": "2024-06-15T12:41:18.167371Z"
    },
    "papermill": {
     "duration": 0.038189,
     "end_time": "2024-06-15T12:41:18.169969",
     "exception": false,
     "start_time": "2024-06-15T12:41:18.131780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    PRIVATE = True\n",
    "else:\n",
    "    PRIVATE = False\n",
    "\n",
    "if not PRIVATE:\n",
    "    class train_env():\n",
    "        def __init__(self, randomize=False):\n",
    "            self.randomlize = randomize\n",
    "            \n",
    "            self.df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "            self.df['ground_truth'] = self.df['answer']\n",
    "            self.df['answer'] = -1\n",
    "            \n",
    "            if self.randomlize:\n",
    "                self.df = self.df.reset_index().sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            self.predict_called = True\n",
    "            self.counter = 0\n",
    "            self.len = len(self.df)\n",
    "        \n",
    "        \n",
    "        def iter_test(self):\n",
    "             while self.counter<self.len:\n",
    "                if self.predict_called:\n",
    "                    self.predict_called = False\n",
    "                    yield (self.df.loc[[self.counter]][['id','problem']]),(self.df.loc[[self.counter]][['id','answer']])\n",
    "                else:\n",
    "                    print(\"You must call `predict()` successfully before you can continue with `iter_test()`\")\n",
    "                    yield None \n",
    "                \n",
    "        def predict(self, answer):\n",
    "            self.df.loc[self.counter, ('answer')] = answer['answer'].values[0]\n",
    "            self.predict_called = True\n",
    "            self.counter+=1\n",
    "\n",
    "    env = train_env(randomize=True)\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    # Set up the evaluation API\n",
    "    import aimo\n",
    "\n",
    "    env = aimo.make_env()\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d628c02",
   "metadata": {
    "papermill": {
     "duration": 0.006467,
     "end_time": "2024-06-15T12:41:18.183148",
     "exception": false,
     "start_time": "2024-06-15T12:41:18.176681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8f6013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:18.197610Z",
     "iopub.status.busy": "2024-06-15T12:41:18.197360Z",
     "iopub.status.idle": "2024-06-15T12:41:29.259141Z",
     "shell.execute_reply": "2024-06-15T12:41:29.258323Z"
    },
    "papermill": {
     "duration": 11.07163,
     "end_time": "2024-06-15T12:41:29.261494",
     "exception": false,
     "start_time": "2024-06-15T12:41:18.189864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 12:41:20.068058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-15 12:41:20.068156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-15 12:41:20.195253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "QUANT = False\n",
    "USE_PAST_KEY = True\n",
    "SEED = 42\n",
    "MODEL_PATH = \"/kaggle/input/deepseek-math-7b-instruct\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_REPETITIONS = 19 if PRIVATE else 4\n",
    "MAX_NEW_TOKENS = 2048 if PRIVATE else 512\n",
    "TIME_LIMIT = 31500 if PRIVATE else 1\n",
    "transformers.set_seed(SEED)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "DEVICE_MAP = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 1),\n",
    "                 ('model.layers.19', 1),\n",
    "                 ('model.layers.20', 1),\n",
    "                 ('model.layers.21', 1),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                ('model.layers.30', 1),\n",
    "                  ('model.layers.31', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "DEVICE_MAP = {ii:jj for (ii,jj) in DEVICE_MAP}\n",
    "\n",
    "TEMPERATURE = [0.9, 0.9] # temperature, temperature_coding\n",
    "TOP_P = [1.0, 1.0] # top_p, top_p_coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228a32a",
   "metadata": {
    "papermill": {
     "duration": 0.00669,
     "end_time": "2024-06-15T12:41:29.275423",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.268733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Important Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebf15bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:29.290340Z",
     "iopub.status.busy": "2024-06-15T12:41:29.289824Z",
     "iopub.status.idle": "2024-06-15T12:41:29.298754Z",
     "shell.execute_reply": "2024-06-15T12:41:29.298089Z"
    },
    "papermill": {
     "duration": 0.018505,
     "end_time": "2024-06-15T12:41:29.300642",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.282137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(transformers.StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(DEVICE) for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            last_token = input_ids[0][-len(stop):]\n",
    "            if torch.all(torch.eq(stop,last_token)):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ed35a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:29.315808Z",
     "iopub.status.busy": "2024-06-15T12:41:29.315540Z",
     "iopub.status.idle": "2024-06-15T12:41:29.368616Z",
     "shell.execute_reply": "2024-06-15T12:41:29.367827Z"
    },
    "papermill": {
     "duration": 0.063095,
     "end_time": "2024-06-15T12:41:29.370491",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.307396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM_SYSTEM:\n",
    "    \n",
    "    def __init__(self, model_path, device_map, temperature, top_p, prompt_options):\n",
    "        #init llm\n",
    "        self.model, self.tokenizer = self.initialize_llm(model_path, device_map)\n",
    "        #init stop words\n",
    "        self.stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n",
    "        self.stop_words_ids = [self.tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in self.stop_words]\n",
    "        self.stopping_criteria = transformers.StoppingCriteriaList([StoppingCriteriaSub(stops=self.stop_words_ids)])\n",
    "        \n",
    "        self.prompt_options = prompt_options\n",
    "        \n",
    "        self.temperature = temperature[0]\n",
    "        self.top_p = top_p[0]\n",
    "\n",
    "        self.temperature_coding = temperature[1]\n",
    "        self.top_p_coding = top_p[1]\n",
    "\n",
    "   \n",
    "        self.total_results = {}\n",
    "        self.total_answers = {}\n",
    "        self.best_stats = {}\n",
    "        self.total_outputs = {}\n",
    "        self.question_type_counts = {}\n",
    "        self.starting_counts = (2,3)\n",
    "        self.problem_count = 0\n",
    "        \n",
    "        self.already_generated_length = 0\n",
    "        self.code_error = None\n",
    "        self.code_error_count = 0\n",
    "        self.code_output = -1\n",
    "#====================================================================================#\n",
    "    def initialize_llm(self, model_path, device_map):\n",
    "        config = transformers.AutoConfig.from_pretrained(model_path)\n",
    "        config.gradient_checkpointing = True\n",
    "\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        if QUANT:\n",
    "            quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"sequential\",\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=quantization_config,\n",
    "                config=config\n",
    "            )\n",
    "        else:\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "        return model, tokenizer\n",
    "#====================================================================================#    \n",
    "    def predict(self, problem):\n",
    "        self.problem_count += 1\n",
    "        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "    \n",
    "        if TIME_SPENT>TIME_LIMIT:\n",
    "            return 0\n",
    "\n",
    "        for repetition in tqdm(range(N_REPETITIONS)):\n",
    "            print(f\"\\n\\n\\nQUESTION {self.problem_count} - {repetition} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "            best, best_count = self.best_stats.get(self.problem_count,(-1,-1))\n",
    "            if best_count>np.sqrt(repetition):\n",
    "                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "                continue\n",
    "\n",
    "            outputs = self.total_outputs.get(self.problem_count,[])\n",
    "            text_answers, code_answers = self.question_type_counts.get(self.problem_count,self.starting_counts)\n",
    "            results = self.total_results.get(self.problem_count,[])\n",
    "            answers = self.total_answers.get(self.problem_count,[])  \n",
    "\n",
    "            for _ in range(5):\n",
    "                self.flush()\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            try:\n",
    "                self.already_generated_length = 0\n",
    "                self.code_error = None\n",
    "                self.code_error_count = 0\n",
    "                self.code_output = -1\n",
    "                \n",
    "                counts = np.array([text_answers,code_answers])\n",
    "\n",
    "                draw = np.random.choice(self.prompt_options, 1,\n",
    "                              p=counts/counts.sum())\n",
    "\n",
    "                initial_message = draw[0].format(problem,\"{}\")            \n",
    "                prompt = f\"User: {initial_message}\"\n",
    "\n",
    "                prompt_original_length = len(prompt)\n",
    "                print(f\"{repetition}_{prompt}\\n\")\n",
    "\n",
    "                model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                prompt_token_length = len(model_inputs['input_ids'][0])\n",
    "\n",
    "                generation_output = self.model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length,\n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = self.temperature,\n",
    "                                                   top_p = self.top_p,\n",
    "                                                   num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                print(f\"{decoded_output[prompt_original_length:]}\\n\")\n",
    "                prompt_original_length += len(decoded_output[prompt_original_length:])\n",
    "                cummulative_code = \"\"\n",
    "\n",
    "                stop_word_cond = False\n",
    "                for stop_word in self.stop_words:\n",
    "                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "\n",
    "\n",
    "                while (stop_word_cond) and (self.already_generated_length<(MAX_NEW_TOKENS)):\n",
    "\n",
    "                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n",
    "                        temperature_inner=self.temperature_coding\n",
    "                        top_p_inner = self.top_p_coding\n",
    "                        prompt = decoded_output\n",
    "                    else:\n",
    "                        temperature_inner=self.temperature\n",
    "                        top_p_inner = self.top_p\n",
    "                        try:\n",
    "                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n",
    "                            else:\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n",
    "\n",
    "\n",
    "                            cummulative_code+=code_text\n",
    "                            self.code_output, CODE_STATUS = self.process_code(cummulative_code, return_shell_output=True)\n",
    "                            print('CODE RESULTS', self.code_output)\n",
    "\n",
    "                            if self.code_error==self.code_output:\n",
    "                                self.code_error_count+=1\n",
    "                            else:\n",
    "                                self.code_error=self.code_output\n",
    "                                self.code_error_count = 0\n",
    "\n",
    "                            if not CODE_STATUS:\n",
    "                                cummulative_code = cummulative_code[:-len(code_text)]\n",
    "\n",
    "                                if self.code_error_count>=1:\n",
    "                                    print(\"REPEATED ERRORS\")\n",
    "                                    break\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print('ERROR PARSING CODE')\n",
    "                            self.code_output = -1\n",
    "\n",
    "                        if self.code_output!=-1:\n",
    "                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                                prompt = decoded_output+'```output\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                            else:\n",
    "                                prompt = decoded_output+'\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output\n",
    "                            cummulative_code=\"\"\n",
    "                    model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                    self.already_generated_length =  len(model_inputs['input_ids'][0])-prompt_token_length\n",
    "\n",
    "                    if USE_PAST_KEY:\n",
    "                        old_values = generation_output.past_key_values\n",
    "                    else:\n",
    "                        old_values = None\n",
    "\n",
    "                    generation_output = self.model.generate(**model_inputs, \n",
    "                                                       max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length, \n",
    "                                                       return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                       past_key_values=old_values,\n",
    "                                                       do_sample = True,\n",
    "                                                       temperature = temperature_inner,\n",
    "                                                       top_p = top_p_inner,\n",
    "                                                       num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "                    if USE_PAST_KEY:\n",
    "                        output_ids = generation_output.sequences[0]\n",
    "                    else:\n",
    "                        output_ids = generation_output[0]\n",
    "                    decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[prompt_original_length:]}\\n\")\n",
    "                    prompt_original_length+=len(decoded_output[prompt_original_length:])\n",
    "\n",
    "                    stop_word_cond = False\n",
    "                    for stop_word in self.stop_words:\n",
    "                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "\n",
    "                raw_output = self.tokenizer.decode(output_ids[prompt_token_length:], skip_special_tokens=True)\n",
    "                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "                result_output = self.process_text_output(raw_output)\n",
    "\n",
    "                try:\n",
    "                    self.code_output = round(float(eval(self.code_output))) % 1000\n",
    "                except Exception as e:\n",
    "                    print(e,'final_eval')\n",
    "                    self.code_output = -1\n",
    "            except Exception as e:\n",
    "                print(e,\"5\")\n",
    "                result_output, self.code_output = -1, -1\n",
    "\n",
    "            if self.code_output!=-1:\n",
    "                outputs.append(self.code_output)\n",
    "                code_answers+=1\n",
    "\n",
    "            if result_output!=-1:\n",
    "                outputs.append(result_output)\n",
    "                text_answers+=1\n",
    "\n",
    "            if len(outputs) > 0:\n",
    "                occurences = Counter(outputs).most_common()\n",
    "                print(occurences)\n",
    "                if occurences[0][1] > best_count:\n",
    "                    print(\"GOOD ANSWER UPDATED!\")\n",
    "                    best = occurences[0][0]\n",
    "                    best_count = occurences[0][1]\n",
    "                if occurences[0][1] > 5:\n",
    "                    print(\"ANSWER FOUND!\")\n",
    "                    break\n",
    "\n",
    "            results.append(result_output)\n",
    "            answers.append(self.code_output)\n",
    "\n",
    "            self.best_stats[self.problem_count] = (best, best_count) \n",
    "            self.question_type_counts[self.problem_count] = (text_answers, code_answers)\n",
    "            self.total_outputs[self.problem_count] = outputs\n",
    "\n",
    "            self.total_results[self.problem_count] = results\n",
    "            self.total_answers[self.problem_count] = answers\n",
    "\n",
    "            print(\"code_answers\",code_answers-self.starting_counts[1],\"text_answers\",text_answers-self.starting_counts[0])\n",
    "        return self.best_stats[self.problem_count][0]\n",
    "#====================================================================================#\n",
    "    def flush(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "#====================================================================================#\n",
    "    def naive_parse(self, answer):\n",
    "        out = []\n",
    "        start = False\n",
    "        end = False\n",
    "        for l in reversed(list(answer)):\n",
    "            if l in '0123456789' and not end:\n",
    "                start = True\n",
    "                out.append(l)\n",
    "            else:\n",
    "                if start:\n",
    "                    end = True\n",
    "\n",
    "        out = reversed(out)\n",
    "        return ''.join(out)\n",
    "#====================================================================================#\n",
    "    def return_last_print(self, output, n):\n",
    "        lines = output.strip().split('\\n')\n",
    "        if lines:\n",
    "            return lines[n]\n",
    "        else:\n",
    "            return \"\"\n",
    "#====================================================================================#\n",
    "    def repl(self, match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "#====================================================================================#        \n",
    "    def process_code(self, code, return_shell_output=False):\n",
    "    \n",
    "        code = re.sub(r\"symbols\\([^)]+\\)\", self.repl, code)\n",
    "\n",
    "        if return_shell_output:\n",
    "            code = code.replace('\\n', '\\n    ')\n",
    "                # Add a try...except block\n",
    "            code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n",
    "\n",
    "        if not return_shell_output:\n",
    "            print(code)\n",
    "        with open('code.py', 'w') as fout:\n",
    "            fout.write(code)\n",
    "\n",
    "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "        try:\n",
    "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "            return_value = self.return_last_print(shell_output, -1)\n",
    "            print(shell_output)\n",
    "            if return_shell_output:\n",
    "                if return_value=='FAIL':\n",
    "                    CODE_STATUS = False\n",
    "                    return_value = self.return_last_print(shell_output, -2)\n",
    "                    if \"not defined\" in return_value:\n",
    "                        return_value+='\\nTry checking the formatting and imports'\n",
    "                else:\n",
    "                    CODE_STATUS = True\n",
    "                return return_value, CODE_STATUS  \n",
    "            self.code_output = round(float(eval(return_value))) % 1000\n",
    "        except Exception as e:\n",
    "            print(e,'shell_output')\n",
    "            self.code_output = -1\n",
    "\n",
    "        if return_shell_output:\n",
    "            if self.code_output==-1:\n",
    "                CODE_STATUS = False\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return self.code_output, CODE_STATUS  \n",
    "\n",
    "\n",
    "        return self.code_output\n",
    "#====================================================================================#    \n",
    "    def process_text_output(self, output):\n",
    "        result = output    \n",
    "        try:\n",
    "            result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "\n",
    "            print('BOXED', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = self.naive_parse(result)\n",
    "            else:\n",
    "                result_output = result_output[-1]\n",
    "\n",
    "            print('BOXED FINAL', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = -1\n",
    "\n",
    "            else:\n",
    "                result_output = round(float(eval(result_output))) % 1000\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('ERROR PARSING TEXT')\n",
    "            result_output = -1\n",
    "\n",
    "        return result_output\n",
    "#====================================================================================#\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a9435",
   "metadata": {
    "papermill": {
     "duration": 0.006465,
     "end_time": "2024-06-15T12:41:29.383884",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.377419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f8fe8",
   "metadata": {
    "papermill": {
     "duration": 0.006476,
     "end_time": "2024-06-15T12:41:29.396981",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.390505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7dee3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:29.411613Z",
     "iopub.status.busy": "2024-06-15T12:41:29.411339Z",
     "iopub.status.idle": "2024-06-15T12:41:29.416352Z",
     "shell.execute_reply": "2024-06-15T12:41:29.415529Z"
    },
    "papermill": {
     "duration": 0.014506,
     "end_time": "2024-06-15T12:41:29.418235",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.403729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n",
    "\\\"{}\\\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\n",
    "Approach:\"\"\"\n",
    "\n",
    "\n",
    "cot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n",
    "\\\"{}\\\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n",
    "\n",
    "prompt_options = [code,cot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b846fd",
   "metadata": {
    "papermill": {
     "duration": 0.006571,
     "end_time": "2024-06-15T12:41:29.431516",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.424945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e2c1ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:41:29.446129Z",
     "iopub.status.busy": "2024-06-15T12:41:29.445682Z",
     "iopub.status.idle": "2024-06-15T12:43:37.169403Z",
     "shell.execute_reply": "2024-06-15T12:43:37.168456Z"
    },
    "papermill": {
     "duration": 127.733268,
     "end_time": "2024-06-15T12:43:37.171534",
     "exception": false,
     "start_time": "2024-06-15T12:41:29.438266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e51484d1424e94b1e42f8a7ed4c132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "llm = LLM_SYSTEM(MODEL_PATH, DEVICE_MAP, TEMPERATURE, TOP_P, prompt_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700ce99",
   "metadata": {
    "papermill": {
     "duration": 0.006968,
     "end_time": "2024-06-15T12:43:37.186006",
     "exception": false,
     "start_time": "2024-06-15T12:43:37.179038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31ec9a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:43:37.201599Z",
     "iopub.status.busy": "2024-06-15T12:43:37.201324Z",
     "iopub.status.idle": "2024-06-15T12:43:37.252678Z",
     "shell.execute_reply": "2024-06-15T12:43:37.251663Z"
    },
    "papermill": {
     "duration": 0.0615,
     "end_time": "2024-06-15T12:43:37.254729",
     "exception": false,
     "start_time": "2024-06-15T12:43:37.193229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                            problem\n",
      "0  739bc9  For how many positive integers $m$ does the eq...\n",
      "       id  answer\n",
      "0  739bc9       0 \n",
      "\n",
      "       id                                            problem\n",
      "1  82e2a0  Suppose that we roll four 6-sided fair dice wi...\n",
      "       id  answer\n",
      "1  82e2a0       0 \n",
      "\n",
      "       id                                            problem\n",
      "2  bedda4  Let $ABCD$ be a unit square. Let $P$ be the po...\n",
      "       id  answer\n",
      "2  bedda4       0 \n",
      "\n",
      "       id                                            problem\n",
      "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...\n",
      "       id  answer\n",
      "3  430b63       0 \n",
      "\n",
      "       id                                            problem\n",
      "4  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...\n",
      "       id  answer\n",
      "4  229ee8       0 \n",
      "\n",
      "       id                                            problem\n",
      "5  246d26  Each of the three-digits numbers $111$ to $999...\n",
      "       id  answer\n",
      "5  246d26       0 \n",
      "\n",
      "       id                                            problem\n",
      "6  d7e9c9  A function $f: \\mathbb N \\to \\mathbb N$ satisf...\n",
      "       id  answer\n",
      "6  d7e9c9       0 \n",
      "\n",
      "       id                                            problem\n",
      "7  5277ed  There exists a unique increasing geometric seq...\n",
      "       id  answer\n",
      "7  5277ed       0 \n",
      "\n",
      "       id                                            problem\n",
      "8  2fc4ad  Let the `sparkle' operation on positive intege...\n",
      "       id  answer\n",
      "8  2fc4ad       0 \n",
      "\n",
      "       id                                            problem\n",
      "9  8ee6f3  The points $\\left(x, y\\right)$ satisfying $((\\...\n",
      "       id  answer\n",
      "9  8ee6f3       0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test, sample_submission in iter_test:\n",
    "    sample_submission['answer'] = llm.predict(test['problem'].values[0])\n",
    "    env.predict(sample_submission)\n",
    "    print(test)\n",
    "    print(sample_submission, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb3379",
   "metadata": {
    "papermill": {
     "duration": 0.006997,
     "end_time": "2024-06-15T12:43:37.269047",
     "exception": false,
     "start_time": "2024-06-15T12:43:37.262050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9069d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T12:43:37.284689Z",
     "iopub.status.busy": "2024-06-15T12:43:37.284411Z",
     "iopub.status.idle": "2024-06-15T12:43:37.394012Z",
     "shell.execute_reply": "2024-06-15T12:43:37.392962Z"
    },
    "papermill": {
     "duration": 0.119651,
     "end_time": "2024-06-15T12:43:37.395954",
     "exception": false,
     "start_time": "2024-06-15T12:43:37.276303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('code.py', 'w') as fout:\n",
    "    fout.write(\"print('done')\")\n",
    "\n",
    "batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "try:\n",
    "    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "    print(shell_output)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef57012",
   "metadata": {
    "papermill": {
     "duration": 0.007233,
     "end_time": "2024-06-15T12:43:37.410625",
     "exception": false,
     "start_time": "2024-06-15T12:43:37.403392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913635,
     "sourceId": 8274980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913643,
     "sourceId": 8274989,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4725625,
     "sourceId": 8023710,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 221.283205,
   "end_time": "2024-06-15T12:43:40.663231",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-15T12:39:59.380026",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "29cf8295df274ecb917dbba382ae37ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2e02c8f92f954d0384abf612fdee7ddb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "420bd53a95684123a9f2140ffda546bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "443b0bbc91734a2da1e8848abfa25db0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "57a8e56cb22d4bf8833f37f4abba422e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_443b0bbc91734a2da1e8848abfa25db0",
       "placeholder": "",
       "style": "IPY_MODEL_29cf8295df274ecb917dbba382ae37ce",
       "value": " 2/2 [02:06&lt;00:00, 60.48s/it]"
      }
     },
     "5cafa7fc84f74a6eaf917e731a670b16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5f46c527d0064e6484719f73f76119a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a457ecbfde6f4c0ea5385bb3b978d1b8",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5cafa7fc84f74a6eaf917e731a670b16",
       "value": 2.0
      }
     },
     "7ad4671bf3e341839db8d601988db972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a457ecbfde6f4c0ea5385bb3b978d1b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "abe464c2d0c44b3bb448e039100a750f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2e02c8f92f954d0384abf612fdee7ddb",
       "placeholder": "",
       "style": "IPY_MODEL_7ad4671bf3e341839db8d601988db972",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b7e51484d1424e94b1e42f8a7ed4c132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_abe464c2d0c44b3bb448e039100a750f",
        "IPY_MODEL_5f46c527d0064e6484719f73f76119a9",
        "IPY_MODEL_57a8e56cb22d4bf8833f37f4abba422e"
       ],
       "layout": "IPY_MODEL_420bd53a95684123a9f2140ffda546bf"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
